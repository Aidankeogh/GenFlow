{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c4c7e8-71b1-4218-84fe-26c6707a49e0",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76080d79-6a00-43e6-b701-09db9f49070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "def pretty_print(d):\n",
    "    print(f'=======\\n{json.dumps(d, sort_keys=True, indent=4)}\\n======')\n",
    "\n",
    "home_dir = os.path.expanduser('~')\n",
    "mlflow_dir = os.path.join(home_dir, 'git', 'rattaca', 'app', 'flask', 'mlruns')\n",
    "mlflow.set_tracking_uri(f'file://{mlflow_dir}')\n",
    "# print(mlflow.get_tracking_uri())\n",
    "\n",
    "# create_expiriments = False\n",
    "# if create_expiriments:\n",
    "iris_exp = mlflow.create_experiment(\"iris\")\n",
    "housing_exp = mlflow.create_experiment(\"housing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dbabd3-5e54-4466-b499-1203d3c61da3",
   "metadata": {},
   "source": [
    "### Classifier Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baf9786-92be-4180-b298-e88c8d89af38",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def run_iris(run_name, test_size, n_estimators):\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "    df['target'] = iris.target\n",
    "    df.to_csv('iris_data.csv')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[iris.feature_names], iris.target, test_size=test_size, random_state=42)\n",
    "\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    eval_data = X_test\n",
    "    eval_data['label'] = y_test\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, description='iris sample', experiment_id=iris_exp):\n",
    "\n",
    "        mlflow.sklearn.log_model(rf, 'iris_model')\n",
    "        model_uri = mlflow.get_artifact_uri('iris_model')\n",
    "        \n",
    "        mlflow.log_params({**rf.get_params(), 'test_size': test_size, 'n_estimators': n_estimators})\n",
    "        \n",
    "        \n",
    "        result = mlflow.evaluate(\n",
    "            model_uri,\n",
    "            eval_data,\n",
    "            targets=\"label\",\n",
    "            model_type=\"classifier\",\n",
    "            evaluators=[\"default\"],\n",
    "        )\n",
    "        pretty_print(result.metrics)\n",
    "\n",
    "\n",
    "run_iris('iris_1', 0.15, 1)\n",
    "run_iris('iris_2', 0.30, 10)\n",
    "run_iris('iris_3', 0.15, 1)\n",
    "run_iris('iris_4', 0.30, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4af074a-0f84-4424-9d43-f449c1d638a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:///Users/leonardoclo/git/rattaca/app/flask/mlruns\n"
     ]
    }
   ],
   "source": [
    "print(mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47548780-0407-4b58-b864-dfea0eb05eb3",
   "metadata": {},
   "source": [
    "### Regresion example (With custom metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9114e0d8-9283-430a-a9cf-b84b7a45cf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/04/28 11:20:45 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/04/28 11:20:45 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/04/28 11:20:45 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'LinearRegression' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "2023/04/28 11:20:49 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/04/28 11:20:49 INFO mlflow.models.evaluation.default_evaluator: Shap explainer Linear is used.\n",
      "Unable to serialize underlying model using MLflow, will use SHAP serialization\n",
      "2023/04/28 11:20:50 WARNING mlflow.models.evaluation.default_evaluator: Logging explainer failed. Reason: AttributeError(\"'LinearRegression' object has no attribute 'save'\"). Set logging level to DEBUG to see the full traceback.\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n",
      "No data for colormapping provided via 'c'. Parameters 'vmin', 'vmax' will be ignored\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models import make_metric\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def squared_diff_plus_one(eval_df, _builtin_metrics):\n",
    "    \"\"\"\n",
    "    This example custom metric function creates a metric based on the ``prediction`` and\n",
    "    ``target`` columns in ``eval_df`.\n",
    "    \"\"\"\n",
    "    return np.sum(np.abs(eval_df[\"prediction\"] - eval_df[\"target\"] + 1) ** 2)\n",
    "\n",
    "\n",
    "def sum_on_target_divided_by_two(_eval_df, builtin_metrics):\n",
    "    \"\"\"\n",
    "    This example custom metric function creates a metric derived from existing metrics in\n",
    "    ``builtin_metrics``.\n",
    "    \"\"\"\n",
    "    return builtin_metrics[\"sum_on_target\"] / 2\n",
    "\n",
    "\n",
    "def prediction_target_scatter(eval_df, _builtin_metrics, artifacts_dir):\n",
    "    \"\"\"\n",
    "    This example custom artifact generates and saves a scatter plot to ``artifacts_dir`` that\n",
    "    visualizes the relationship between the predictions and targets for the given model to a\n",
    "    file as an image artifact.\n",
    "    \"\"\"\n",
    "    plt.scatter(eval_df[\"prediction\"], eval_df[\"target\"])\n",
    "    plt.xlabel(\"Targets\")\n",
    "    plt.ylabel(\"Predictions\")\n",
    "    plt.title(\"Targets vs. Predictions\")\n",
    "    plot_path = os.path.join(artifacts_dir, \"example_scatter_plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    return {\"example_scatter_plot_artifact\": plot_path}\n",
    "\n",
    "\n",
    "def run_housing(run_name, test_size):\n",
    "    # loading the California housing dataset\n",
    "    cali_housing = fetch_california_housing(as_frame=True)\n",
    "    df = pd.DataFrame(data=cali_housing.data, columns=cali_housing.feature_names)\n",
    "    df['target'] = cali_housing.target\n",
    "    df.to_csv('cali_housing.csv')\n",
    "\n",
    "    # split the dataset into train and test partitions\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        cali_housing.data, cali_housing.target, test_size=test_size, random_state=123\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    lin_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "    # creating the evaluation dataframe\n",
    "    eval_data = X_test.copy()\n",
    "    eval_data[\"target\"] = y_test\n",
    "    with mlflow.start_run(run_name=run_name, experiment_id=housing_exp, description='hosing sample'):\n",
    "        mlflow.sklearn.log_model(lin_reg, \"model\")\n",
    "        model_uri = mlflow.get_artifact_uri(\"model\")\n",
    "\n",
    "        mlflow.log_params({**lin_reg.get_params(), 'test_size': test_size})\n",
    "\n",
    "        result = mlflow.evaluate(\n",
    "            model=model_uri,\n",
    "            data=eval_data,\n",
    "            targets=\"target\",\n",
    "            model_type=\"regressor\",\n",
    "            evaluators=[\"default\"],\n",
    "            custom_metrics=[\n",
    "                make_metric(\n",
    "                    eval_fn=squared_diff_plus_one,\n",
    "                    greater_is_better=False,\n",
    "                ),\n",
    "                make_metric(\n",
    "                    eval_fn=sum_on_target_divided_by_two,\n",
    "                    greater_is_better=True,\n",
    "                ),\n",
    "            ],\n",
    "            custom_artifacts=[prediction_target_scatter],\n",
    "        )\n",
    "\n",
    "run_housing('housing_1', 0.15)\n",
    "run_housing('housing_2', 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06da66c1-b365-4f4d-a6df-8e90f6b820b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
