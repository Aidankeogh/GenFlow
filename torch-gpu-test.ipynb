{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55a2f705-13b4-4d50-8a26-ec4f7308b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch import Tensor\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5069d1a2-7252-4e68-b74e-13f3f400812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])\n",
    "\n",
    "data_loader = DataLoader(MNIST('data', train=True, download=True, transform=mnist_transform), batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e9f4cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "66058106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF_block(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.fc2 = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, d_model: int, n_numbers: int, num_heads = 4, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedder = nn.Sequential(\n",
    "            nn.Linear(1, d_model), \n",
    "            PositionalEncoding(d_model)\n",
    "        )  \n",
    "        queries = torch.randn(1, n_numbers, d_model)\n",
    "        self.register_parameter('queries', nn.Parameter(queries))\n",
    "        self.mh_attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "        self.key = FF_block(d_model)\n",
    "        self.value = FF_block(d_model)\n",
    "\n",
    "        self.cls_head = nn.Linear(d_model, n_classes)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = self.embedder(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        queries = self.queries.repeat(x.shape[0], 1, 1)\n",
    "        x, _ = self.mh_attention(queries, key, value)\n",
    "        x = self.cls_head(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4845580",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "d_model = 64\n",
    "n_numbers = 4\n",
    "model = CrossAttentionModel(d_model, n_numbers)\n",
    "#model = nn.Linear(n_numbers * 28 * 28, n_numbers * 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d40585a7-08a9-4899-84f7-ba18ce7b6cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299724\n",
      "Train Epoch: 0 [1600/60000 (11%)]\tLoss: 2.301477\n",
      "Train Epoch: 0 [3200/60000 (21%)]\tLoss: 2.300746\n",
      "Train Epoch: 0 [4800/60000 (32%)]\tLoss: 2.299948\n",
      "Train Epoch: 0 [6400/60000 (43%)]\tLoss: 2.300999\n",
      "Train Epoch: 0 [8000/60000 (53%)]\tLoss: 2.296153\n",
      "Train Epoch: 0 [9600/60000 (64%)]\tLoss: 2.296517\n",
      "Train Epoch: 0 [11200/60000 (75%)]\tLoss: 2.296639\n",
      "Train Epoch: 0 [12800/60000 (85%)]\tLoss: 2.298007\n",
      "Train Epoch: 0 [14400/60000 (96%)]\tLoss: 2.308249\n",
      "\n",
      "Accuracy: 6726/60000 (11%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.315595\n",
      "Train Epoch: 1 [1600/60000 (11%)]\tLoss: 2.297168\n",
      "Train Epoch: 1 [3200/60000 (21%)]\tLoss: 2.297946\n",
      "Train Epoch: 1 [4800/60000 (32%)]\tLoss: 2.309592\n",
      "Train Epoch: 1 [6400/60000 (43%)]\tLoss: 2.303154\n",
      "Train Epoch: 1 [8000/60000 (53%)]\tLoss: 2.291953\n",
      "Train Epoch: 1 [9600/60000 (64%)]\tLoss: 2.310157\n",
      "Train Epoch: 1 [11200/60000 (75%)]\tLoss: 2.292486\n",
      "Train Epoch: 1 [12800/60000 (85%)]\tLoss: 2.298386\n",
      "Train Epoch: 1 [14400/60000 (96%)]\tLoss: 2.291788\n",
      "\n",
      "Accuracy: 6743/60000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.303535\n",
      "Train Epoch: 2 [1600/60000 (11%)]\tLoss: 2.311386\n",
      "Train Epoch: 2 [3200/60000 (21%)]\tLoss: 2.306528\n",
      "Train Epoch: 2 [4800/60000 (32%)]\tLoss: 2.304566\n",
      "Train Epoch: 2 [6400/60000 (43%)]\tLoss: 2.301238\n",
      "Train Epoch: 2 [8000/60000 (53%)]\tLoss: 2.276947\n",
      "Train Epoch: 2 [9600/60000 (64%)]\tLoss: 2.268225\n",
      "Train Epoch: 2 [11200/60000 (75%)]\tLoss: 2.284585\n",
      "Train Epoch: 2 [12800/60000 (85%)]\tLoss: 2.224975\n",
      "Train Epoch: 2 [14400/60000 (96%)]\tLoss: 2.284127\n",
      "\n",
      "Accuracy: 7453/60000 (12%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.289332\n",
      "Train Epoch: 3 [1600/60000 (11%)]\tLoss: 2.307713\n",
      "Train Epoch: 3 [3200/60000 (21%)]\tLoss: 2.290884\n",
      "Train Epoch: 3 [4800/60000 (32%)]\tLoss: 2.238477\n",
      "Train Epoch: 3 [6400/60000 (43%)]\tLoss: 2.288712\n",
      "Train Epoch: 3 [8000/60000 (53%)]\tLoss: 2.284239\n",
      "Train Epoch: 3 [9600/60000 (64%)]\tLoss: 2.275389\n",
      "Train Epoch: 3 [11200/60000 (75%)]\tLoss: 2.301044\n",
      "Train Epoch: 3 [12800/60000 (85%)]\tLoss: 2.277861\n",
      "Train Epoch: 3 [14400/60000 (96%)]\tLoss: 2.275918\n",
      "\n",
      "Accuracy: 8846/60000 (15%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.268244\n",
      "Train Epoch: 4 [1600/60000 (11%)]\tLoss: 2.287106\n",
      "Train Epoch: 4 [3200/60000 (21%)]\tLoss: 2.252594\n",
      "Train Epoch: 4 [4800/60000 (32%)]\tLoss: 2.272475\n",
      "Train Epoch: 4 [6400/60000 (43%)]\tLoss: 2.279033\n",
      "Train Epoch: 4 [8000/60000 (53%)]\tLoss: 2.236999\n",
      "Train Epoch: 4 [9600/60000 (64%)]\tLoss: 2.273533\n",
      "Train Epoch: 4 [11200/60000 (75%)]\tLoss: 2.255437\n",
      "Train Epoch: 4 [12800/60000 (85%)]\tLoss: 2.258275\n",
      "Train Epoch: 4 [14400/60000 (96%)]\tLoss: 2.301749\n",
      "\n",
      "Accuracy: 9008/60000 (15%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.254731\n",
      "Train Epoch: 5 [1600/60000 (11%)]\tLoss: 2.269841\n",
      "Train Epoch: 5 [3200/60000 (21%)]\tLoss: 2.291915\n",
      "Train Epoch: 5 [4800/60000 (32%)]\tLoss: 2.309685\n",
      "Train Epoch: 5 [6400/60000 (43%)]\tLoss: 2.294614\n",
      "Train Epoch: 5 [8000/60000 (53%)]\tLoss: 2.266102\n",
      "Train Epoch: 5 [9600/60000 (64%)]\tLoss: 2.260803\n",
      "Train Epoch: 5 [11200/60000 (75%)]\tLoss: 2.303356\n",
      "Train Epoch: 5 [12800/60000 (85%)]\tLoss: 2.239965\n",
      "Train Epoch: 5 [14400/60000 (96%)]\tLoss: 2.283609\n",
      "\n",
      "Accuracy: 9398/60000 (16%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.267757\n",
      "Train Epoch: 6 [1600/60000 (11%)]\tLoss: 2.282226\n",
      "Train Epoch: 6 [3200/60000 (21%)]\tLoss: 2.288209\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.view(-1, n_numbers * 28 * 28, 1)\n",
    "        output = model(data)\n",
    "        output = output.view(-1, 10)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            #print(output[0].tolist(), target[0])\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(data_loader.dataset),\n",
    "                100. * batch_idx / len(data_loader), loss.item()))\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4e12470a6bd1ddf239ef9d2cbb0fe9fada8c3415f77ba30801c7b104792fc14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
